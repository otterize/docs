---
sidebar_position: 5
title: Egress Policy Automation
image: /img/quick-tutorials/egress-access-control/social.png
---
Let’s learn how Otterize automates egress access control with network policies

In this tutorial, we will:

- Deploy an example cluster consisting of an example frontend for a personal advice application and a server with an external dependency to retrieve wisdom.
- Declare our network intents for each pod, including public internet and internal network egress rules.
- See that a network policy was autogenerated to allow just that and block the (undeclared) calls from the other client.

## Prerequisites
### Install Otterize on your cluster
To deploy Otterize, head over to [Otterize Cloud](https://app.otterize.com/), and to integrate your cluster, navigate to the [Clusters page](https://app.otterize.com/clusters) and follow the instructions, but be sure to add the flag below.

**Note:**  Egress policy creation is off by default. We must add the following flag when installing Otterize to enable egress policy creation.
```bash
--set intentsOperator.operator.enableEgressNetworkPolicyCreation=true
```

## Tutorial
### Deploy the cluster

This will set up the namespace we will use for our tutorial and deploy the cluster containing our front and backend pods. Upon deployment, our cluster will have no network policies in place.

```yaml
kubectl create namespace otterize-tutorial-egress
kubectl apply -n otterize-tutorial-egress -f ${ABSOLUTE_URL}/code-examples/network-egress/all.yaml
```

### About Network Policies

By default, in Kubernetes, pods are non-isolated. Meaning they accept traffic from any source and can send traffic to any source. When you introduce policies, either ingress or egress pods become isolated. Any connection not explicitly allowed will be rejected. When an ingress policy type is introduced, any traffic that does not match a rule will be rejected. Similarly, when an egress policy type is introduced, any traffic that does not match a rule will not be allowed out of the pod.

Stringent policies can be essential in certain sectors, such as healthcare, finance, and government. Implementing egress policies is crucial in minimizing attack surfaces by concealing services or restricting the exposure of any compromised services. However, challenges may emerge when egress policies are applied to services dependent on external communications that were not initially accounted for. These external communications could include DNS, time synchronization, package repositories, logging, telemetry, cloud services, authentication, or other critical external dependencies that, while not directly related to a pod's primary functionality, are vital for its operation.

Otterize helps elevate these issues by capturing and mapping the ingress and egress connections used by your pods and then providing suggested policies to maintain your witnessed tariff. You can also enable [shadow enforcement](/reference/shadow-vs-active-enforcement) to see which connections would be blocked without committing to active enforcement.

### Defining our intents

We aim to secure the network in our example cluster by introducing a default deny policy for our entire network and policies for each pod’s appropriate ingress and egress needs.

* Frontend - Needs to retrieve advice from our backend. This will result in an egress policy on our frontend and an ingress policy on our backend.
* Backend - Needs to be able to accept our frontend request and communicate to an external API. This will create an ingress policy for our frontend and an egress policy for the external API.

As previously mentioned, the pods will be non-isolated by default, and everything will work. Can you open up the logs on the frontend to see the free advice flowing:

```bash
kubectl logs -f -n otterize-tutorial-egress-access deploy/frontend
```

### Applying our intents

Given that this is a serious advice application, we want to lock down our pods to ensure no outside inference can occur.

To enforce the strict communication rules for our services, we will start by applying a default deny policy, ensuring that only explicitly defined connections are allowed.  You’ll see that we are allowing UDP on port 53 to support any DNS lookups we need.

```bash
kubectl apply -n otterize-tutorial-egress-access -f ${ABSOLUTE_URL}/code-examples/egress-access-control/default-deny-policy.yaml
```

*Default Deny Policy*
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Egress
  - Ingress
  egress:
      - ports:
        - protocol: UDP
          port: 53

```


You can now see in the logs that the pods are isolated from each other and the public internet:

```bash
kubectl logs -f -n otterize-tutorial-egress-access deploy/frontend
```

Now that we have secured our broader network, we will apply the following ClientIntents to enable traffic for our services.

```bash
kubectl apply -n otterize-tutorial-egress-access -f ${ABSOLUTE_URL}/code-examples/egress-access-control/intents.yaml
```

```yaml
apiVersion: k8s.otterize.com/v1alpha3
kind: ClientIntents
metadata:
  name: frontend
  namespace:  otterize-tutorial-egress-access
spec:
  service:
    name: frontend
  calls:
    - name: backend
---
apiVersion: k8s.otterize.com/v1alpha3
kind: ClientIntents
metadata:
  name: backend
  namespace: otterize-tutorial-egress-access
spec:
  service:
    name: backend
  calls:
    - type: internet
      internet:
        ips:
          - 185.53.57.80 # IP address of our external API
    - name: frontend
```

Now, our network and our services are only able to open connections to those internal and external resources that are explicitly needed.

The protected network can be seen on Otterize Cloud:
<img className="tw-w-128 tw-mb-4" src="/img/quick-tutorials/egress-access-control/cluster-intents-applied.png"/>



## Teardown

To remove the deployed examples, run:

```*bash*
kubectl delete namespace otterize-tutorial-egress-access
```







