---
sidebar_position: 5
title: Egress Policy Automation
image: /img/visualization/k8s-network-mapper/social.png
---
Let’s learn how Otterize automates egress access control with network policies

In this tutorial, we will:

- Deploy an example cluster consisting of an example frontend for a quiz game and a server with an external dependency to retrieve questions.
- Declare our network intents for each pod, including public internet and internal network egress rules.
- See that a network policy was autogenerated to allow just that and block the (undeclared) calls from the other client.

## Prerequisites

### Prepare a Kubernetes cluster

For this tutorial, we will need a cluster with a network plugin like Calio or Cilium to enforce our policies. If you already have that enable, you can skip to the next step.
<details>
    <summary>Prepare a Kubernetes cluster with Minikube & Calico</summary>

    For this tutorial you'll need a local Kubernetes cluster. Having a cluster with a [CNI](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) that supports [NetworkPolicies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) isn't required for this tutorial, but is recommended so that your cluster works with other tutorials.

    If you don't have the Minikube CLI, first [install it](https://minikube.sigs.k8s.io/docs/start/).

    Then start your Minikube cluster with Calico, in order to enforce network policies.

    ```shell
    minikube start --cpus=4 --memory 4096 --disk-size 32g --cni=calico
    ```

</details>


### Install Otterize on your cluster
To deploy Otterize, head over to [Otterize Cloud](https://app.otterize.com/), and to integrate your cluster, navigate to the [Clusters page](https://app.otterize.com/clusters) and follow the instructions, but be sure to add the flag below.

**Note:**  Egress policy creation is off by default. We must add the following flag when installing Otterize to enable egress policy creation.
```bash
--set intentsOperator.operator.enableEgressNetworkPolicyCreation=true
```

## Tutorial

### Deploy the cluster

This will set up the namespace we will use for our tutorial and deploy the cluster containing our front and backend pods. Upon deployment, our cluster will have no network policies in place.

```yaml
kubectl create namespace otterize-tutorial-egress
kubectl apply -n otterize-tutorial-egress -f ${ABSOLUTE_URL}/code-examples/network-egress/all.yaml
```

### About Network Policies

By default, in Kubernetes, pods are non-isolated. Meaning they accept traffic from any source and can send traffic to any source. When you introduce policies, either ingress or egress pods become isolated. Any connection not explicitly allowed will be rejected. When an ingress policy type is introduced, any traffic that doesn’t match a rule will be rejected. Similarly, when an egress policy type is introduced, any traffic that doesn’t match a rule will not be allowed out of the pod.

Problems can arise when introducing egress policies on services that rely on external communications, which are not immediately considered. This might be DNS, time, package repositories, logging, telemetry, cloud provider, authentication, or other external core dependencies that might not be related to the direct functionality of the pod.

Otterize helps elevate these issues by capturing and mapping the ingress and egress connections used by your pods and then providing suggested policies to maintain your witnessed tariff. You can also enable [shadow enforcement](/reference/shadow-vs-active-enforcement) to see which connections would be blocked without committing to active enforcement.

### Defining our intents

We aim to secure the network in our example cluster by introducing the appropriate egress policies for each pod.

* Frontend - Needs to retrieve questions from our backend. This will result in an egress policy on our frontend and an ingress policy on our backend.
* Backend - Needs to be able to accept our frontend request and communicate to an external API. This will create an ingress policy for our frontend and an egress policy for the external API.

As previously mentioned, the pods will be non-isolated by default, and everything will work. Can you open up the logs on the frontend to see the questions:

```bash
kubectl logs -f -n otterize-tutorial-egress-access deploy/frontend
```

### Applying our intents

Given this is a serious quiz, we want to lock down our pods to ensure that no outside inference can occur.

To enforce the strict communication rules for our services, we will use the following ClientIntents yaml.

```yaml
apiVersion: k8s.otterize.com/v1alpha3
kind: ClientIntents
metadata:
  name: frontend
  namespace:  otterize-tutorial-egress-access
spec:
  service:
    name: frontend
  calls:
    - name: backend
---
apiVersion: k8s.otterize.com/v1alpha3
kind: ClientIntents
metadata:
  name: backend
  namespace: otterize-tutorial-egress-access
spec:
  service:
    name: backend
  calls:
    - type: internet
      internet:
        ips:
          - 173.236.198.186 # IP address of our external API
    - name: frontend
```

To apply those intents, run the following command:

```bash
kubectl apply -f ${ABSOLUTE_URL}/code-examples/network-egress/intents.yaml
```

Our services are now secure, but to ensure they are being enforced, let’s remove our intent to call our external API.

```bash
kubectl apply -f ${ABSOLUTE_URL}/code-examples/network-egress/intents-no-external-api.yaml
```

Now, if we check our frontend logs, we can see that we are no longer receiving questions.

```bash
kubectl logs -f -n otterize-tutorial-egress-access deploy/frontend
```

We can resolve this by reapplying our previous intents.yaml file.

## Teardown

To remove the deployed examples, run:

```*bash*
kubectl delete namespace otterize-tutorial-egress-access
```



