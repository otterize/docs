---
sidebar_position: 2
title: AWS access for Kubernetes workloads anywhere
image: /img/quick-tutorials/aws-iam-roles-anywhere/social.png
---

Let’s explore how to securely manage access for Kubernetes workloads to AWS services regardless of where they are running (Google Cloud, Azure, on-prem, local) using AWS IAM Roles Anywhere, SPIFFE, Cert Manager, and Otterize.

In this tutorial, we will:

- Deploy a Kubernetes cluster with an example client-server application where the server needs to access AWS S3.
- Use Cert Manager to issue SPIFFE identities for our workloads.
- Utilize Otterize to automate the creation of AWS IAM roles and policies.
- Demonstrate how AWS IAM Roles Anywhere allows our Kubernetes workloads to securely assume IAM roles and access AWS services.

## Prerequisites

### CLI Tools

1. [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html). You will also need credentials within the target account with permission to manage S3 buckets.
2. [cmctl](https://cert-manager.io/docs/reference/cmctl/) a tool that can help you manage cert-manager and its resources inside your cluster.
3. [terraform](https://developer.hashicorp.com/terraform/install) Infrastructure as Code tool we will use to securely integrate Otterize with AWS.
4. [jq](https://jqlang.github.io/jq/download/) For quickly parsing JSON values.

### Kubernetes Cluster

The exciting part is that you can use a cluster hosted in any environment. Feel free to use any cluster you have available. [Minikube](https://minikube.sigs.k8s.io/docs/start/) is a great option to make things easy.

## Tutorial

This tutorial will deploy a cluster that creates files within an S3 bucket. Our cluster is not deployed on [AWS EKS](https://aws.amazon.com/eks/), and we want to manage access using ephemeral credentials. We will learn how to use SPIFFE, cert-manager, and AWS Roles Anywhere to establish short-lived secure access to AWS using Otterize.

### Create S3 Bucket

First, we are going to create our S3 bucket that will store our files from our application

```bash
export BUCKET_NAME=otterize-tutorial-cross-cloud-`date +%s`
echo $BUCKET_NAME
aws s3api create-bucket --bucket $BUCKET_NAME --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2
```

### Deploy Application

Next, we will deploy our example client-server application and inject the bucket’s name using an environmental variable. The server will not initially have access to AWS S3.

```bash
kubectl create namespace otterize-tutorial-cross-cloud
kubectl apply -n otterize-tutorial-cross-cloud -f ${ABSOLUTE_URL}/code-examples/aws-cross-cloud/all.yaml
kubectl patch deployment server -n otterize-tutorial-cross-cloud --type='json' -p="[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/env/-\", \"value\": {\"name\": \"BUCKET_NAME\", \"value\": \"$BUCKET_NAME\"}}]"
```

<details>
    <summary>View Deployment YAML</summary>

```yaml
{@include: ../../../../static/code-examples/aws-cross-cloud/all.yaml}
```
</details>

With everything set up, we can inspect our logs and see that our application cannot access s3. Let’s fix that!

```*bash*
kubectl logs -f -n otterize-tutorial-cross-cloud deploy/server
```
Example log output:
```
2024/03/12 20:44:09 Couldn't upload file testfile.0.txt to otterize-tutorial-cross-cloud-1710270391:testfile.0.txt. Here's why: operation error S3: PutObject, failed to sign request: failed to retrieve credentials: failed to refresh cached credentials, no EC2 IMDS role found, operation error ec2imds: GetMetadata, request canceled, context deadline exceeded
{"time":"2024-03-12T20:44:09.168838764Z","id":"","remote_ip":"10.244.0.5","host":"server","method":"POST","uri":"/upload","user_agent":"Go-http-client/1.1","status":500,"error":"operation error S3: PutObject, failed to sign request: failed to retrieve credentials: failed to refresh cached credentials, no EC2 IMDS role found, operation error ec2imds: GetMetadata, request canceled, context deadline exceeded","latency":5012512836,"latency_human":"5.012512836s","bytes_in":265,"bytes_out":36}
```

### Using AWS Roles Anywhere to connect our cluster to AWS resources

We will now orchestrate several steps to establish a secure and unique identity for our workloads to connect to the S3 bucket. Below is a diagram explaining how the bridge from a non-AWS cluster to AWS resources is created. As an overview, access will be established through the following steps:
1. Establishing a unique ID using SPIFFE that has a corresponding X.509 certificates managed by cert-manager and generated by our certificate authority (CA) for our pods
2. We will establish a shared key with our CA and AWS to validate the certificates
3. Otterize will generate unique AWS roles for our pods that require AWS access
4. When our pods request AWS resources, they will use their certificates to assume their corresponding roles.

![Tutorial Sequence](/img/quick-tutorials/aws-cross-cloud/sequence-diagram.png)

Let’s configure our cluster as illustrated above.

### Deploying & configuring cert-manager
[cert-manager](https://cert-manager.io/) helps issue X.509 certifications to the cluster. For our configuration, we will disable the automated ￼￼`certificateRequest`￼￼ approver and delegate this task to CSI Driver SPIFFE, which we will explore in the next step.

Install cert-manager:
```bash
helm repo add jetstack https://charts.jetstack.io --force-update

helm upgrade -i -n cert-manager cert-manager jetstack/cert-manager \
  --set extraArgs={--controllers='*\,-certificaterequests-approver'} \
  --set installCRDs=true \
  --create-namespace
```

For this tutorial, we will self-sign our certificates using our certificate authority (CA). This signing CA must be manually approved, as we disabled the automated approver within the cert-manager. Note: A self-signed CA should be replaced with a third-party CA for production uses.

Deploy the CA, with the following:
```bash
kubectl apply -f ${ABSOLUTE_URL}/code-examples/aws-cross-cloud/issuer.yaml
```
Approve our CA:
```bash
cmctl approve -n cert-manager \
  $(kubectl get cr -n cert-manager -ojsonpath='{.items[0].metadata.name}')
```


<details>
    <summary>View issuer YAML</summary>

```yaml
{@include: ../../../../static/code-examples/aws-cross-cloud/issuer.yaml}
```
</details>

### Install SPIFFE CSI Driver
Next, we will install the Container Storage Interface (CSI) SPIFFE driver plugin (csi-driver-spiffe). It delivers [SPIFFE](￼) [SVIDs](￼) using X.509 certificate key pairs to be stored on a temporary file system mounted on K8 Pods. We will use a forked version of the plugin, which supports generating an AWS client using the returned SVID. These changes are expected to be merged into the mainline soon.

```bash
helm upgrade -i -n cert-manager cert-manager-csi-driver-spiffe jetstack/cert-manager-csi-driver-spiffe -f ${ABSOLUTE_URL}/code-examples/aws-cross-cloud/values.yaml
```

<details>
    <summary>View Plugin Driver YAML</summary>

```yaml
{@include: ../../../../static/code-examples/aws-cross-cloud/values.yaml}
```
</details>

Now that our cert-manager is deployed, we want to enable the server pod to create CertificateRequests to get a SPIFFE ID with the following command:
```bash
kubectl apply -n otterize-tutorial-cross-cloud -f ${ABSOLUTE_URL}/code-examples/aws-cross-cloud/rbac.yaml
```

<details>
    <summary>View RBAC configuration</summary>

```yaml
{@include: ../../../../static/code-examples/aws-cross-cloud/rbac.yaml}
```
</details>

### Configure AWS to work with Otterize

To build a secure relationship between AWS and Otterize, we must establish a series of roles and a shared key. We will use an [Otterize Terraform](https://registry.terraform.io/modules/otterize/otterize-aws-iam-rolesanywhere/aws/latest) module to do much of the heavy lifting by doing the following items:

* Retrieve your CA’s public key from your Kubernetes cluster and define it as a trust anchor for IAM Roles Anywhere.
* Set up IAM policy, role, and IAM Role Anywhere policy for the Otterize credentials and intents operator.


Download terraform scripts with the following commands:
```bash
mkdir otterize-aws-terraform
cd otterize-aws-terraform
curl http://localhost:3003/code-examples/aws-cross-cloud/tf/main.tf -o main.tf
curl http://localhost:3003/code-examples/aws-cross-cloud/tf/output.tf -o output.tf
curl http://localhost:3003/code-examples/aws-cross-cloud/tf/variables.tf -o variables.tf
```

Create a bridge between AWS and Otterize using terraform:
```bash
terraform init
terraform apply
```



### Install Otterize with AWS integration

Now that we have exchanged keys and set up the necessary roles, we will install Otterize with flags, which will enable AWS access using Terraform’s output values.

We need to extract the ARNs generated from our Terraform and pass them into Otterize with the following commands:
```bash
otterize_intents_operator_role_arn=$(terraform output -raw otterize-intents-operator-role-arn)
otterize_credentials_operator_role_arn=$(terraform output -raw otterize-credentials-operator-role-arn)
trust_anchor_arn=$(terraform output -raw trust-anchor-arn)
otterize_intents_operator_trust_profile_arn=$(terraform output -json otterize-intents-operator-trust-profile-arn | jq -r .arn)
otterize_credentials_operator_trust_profile_arn=$(terraform output -json otterize-credentials-operator-trust-profile-arn | jq -r .arn)
```

Let us now install Otterize with relevant flags and ARNs:
```bash

helm repo add otterize https://helm.otterize.com --force-update

helm upgrade --install otterize otterize/otterize-kubernetes -n otterize-system --create-namespace \
    --set intentsOperator.operator.mode=defaultActive  \
    --set global.aws.enabled=true \
    --set global.aws.region=us-west-2 \
    --set "intentsOperator.aws.roleARN=${otterize_intents_operator_role_arn}" \
    --set "credentialsOperator.aws.roleARN=${otterize_credentials_operator_role_arn}" \
    --set global.aws.rolesAnywhere.enabled=true \
    --set global.aws.rolesAnywhere.trustDomain=spiffe.cert-manager.io \
    --set global.aws.rolesAnywhere.clusterName=otterize-csi-spiffe-demo \
    --set "global.aws.rolesAnywhere.trustAnchorARN=${trust_anchor_arn}" \
    --set "global.aws.rolesAnywhere.intentsOperatorTrustProfileARN=${otterize_intents_operator_trust_profile_arn}" \
    --set "global.aws.rolesAnywhere.credentialsOperatorTrustProfileARN=${otterize_credentials_operator_trust_profile_arn}"
```


### Applying client intents

We have now integrated Otterize with AWS Roles Anywhere, allowing us to define a [client intent](https://docs.otterize.com/reference/IBAC-Overview) providing our server pod access to our S3 bucket.

```bash
kubectl apply -n otterize-tutorial-cross-cloud -f ${ABSOLUTE_URL}/code-examples/aws-cross-cloud/intent.yaml
```

Now that we have provided access to our server’s unique AWS role, we can inspect our logs again and see that our application can access our s3 bucket.

```*bash*
kubectl logs -f -n otterize-tutorial-cross-cloud deploy/server
```

```
{ADD LOGS HERE}
```

### That’s a wrap

This tutorial demonstrated how to securely manage Kubernetes workloads' access to AWS services using AWS IAM Roles Anywhere, Cert Manager, and Otterize. This enhances the security and flexibility of your cloud-native applications and allows them to be deployed anywhere and still access production resources.

## Teardown

To remove AWS resources created, run:

```bash
terraform destroy
```

To clean up the k8 resources created during this tutorial, run:

```*bash*
kubectl delete namespace otterize-tutorial-cross-cloud
```

To remove Terraform scripts, run:

```bash
rm -Rf otterize-aws-terraform
```
