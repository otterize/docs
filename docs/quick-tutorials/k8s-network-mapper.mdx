---
sidebar_position: 2
title: Map your cluster
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The network mapper and the Otterize CLI work in tandem to allow you to map pod-to-pod traffic within your K8s cluster. This tutorial will guide you
through installing Otterize, mapping traffic and tracking changes.

In this tutorial, we will:

- Deploy a Kafka broker, and two clients calling it.
- Map their communication using the network mapper.
- Watch traffic change by deploying another client and another server and re-inspecting with the network mapper.

## Make sure you have a Kubernetes cluster
Before you start, you need to have a Kubernetes cluster. Having a cluster with a [CNI](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) that supports [NetworkPolicies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) isn't required for this tutorial, but is recommended so that your cluster works with other tutorials.

{@include: ../_common/cluster-setup.md}

## Install the network mapper

{@include: ../_common/install-otterize-network-mapper.md}

## Install the Otterize CLI

{@include: ../_common/install-otterize-cli.md}

## Map the cluster

The network mapper starts to sniff traffic and build an in-memory network map as soon as it's installed.
The Otterize CLI allows you to interact with the network mapper to grab a snapshot of current mapped traffic,
reset its state and more. For a complete list of the CLI capabilities read the [CLI command reference](/cli/#network-mapper).

### Deploy demo to simulate traffic

Let's add traffic to the cluster and see how the network mapper tracks it.
Deploy the following simple example &mdash; `client`, `client2` and `server`, communicating over HTTP:
```shell
kubectl apply -n otterize-tutorial-mapper -f https://docs.otterize.com/code-examples/network-mapper/all.yaml
```

### Show mapped traffic

You can view mapped traffic by calling the CLI `list` or `export` commands.
The latter supports multiple output formats, namely `intents` (client intents files) and `json`.
The following shows the CLI output filtered for the namespace (`otterize-tutorial-mapper`)
of the example above.

<Tabs>
  <TabItem value="list" label="List" default>

1. List the pod-to-pod network map built up ("sniffed") so far:

   ```shell
   otterize mapper list -n otterize-tutorial-mapper
   ```
2. For the simple example above, you should see:
   ```shell
   client in namespace otterize-tutorial-mapper calls:
     - server
   client2 in namespace otterize-tutorial-mapper calls:
     - server
   ```

</TabItem>
  <TabItem value="intents" label="Export as intents" default>

1. Export as YAML client intents (the default format) the pod-to-pod network map built up so far:

   ```shell
   otterize mapper export -n otterize-tutorial-mapper
   ```
2. For the simple example above, you should see (concatenated into one YAML file):
   ```yaml
   apiVersion: k8s.k8s.otterize.com/v1
   kind: ClientIntents
   metadata:
     name: client
     namespace: otterize-tutorial-mapper
   spec:
     service:
       name: client
     calls:
       - name: server
   ---
   apiVersion: k8s.k8s.otterize.com/v1
   kind: ClientIntents
   metadata:
     name: server
     namespace: otterize-tutorial-mapper
   spec:
     service:
       name: client2
     calls:
       - name: server
   ```

</TabItem>
  <TabItem value="json" label="Export as JSON">

1. Export as JSON the pod-to-pod network map built up so far:
   ```shell
   otterize mapper export -n otterize-tutorial-mapper --format json
   ```
2. For the simple example above, you should see:

   ```json
   [
     {
       "kind": "ClientIntents",
       "apiVersion": "k8s.otterize.com/v1alpha2",
       "metadata": {
         "name": "client",
         "namespace": "otterize-tutorial-mapper"
       },
       "spec": {
         "service": {
           "name": "client"
         },
         "calls": [
           {
             "name": "server"
           }
         ]
       }
     },
     {
       "kind": "ClientIntents",
       "apiVersion": "k8s.otterize.com/v1alpha2",
       "metadata": {
         "name": "client",
         "namespace": "otterize-tutorial-mapper"
       },
       "spec": {
         "service": {
           "name": "client"
         },
         "calls": [
           {
             "name": "server"
           }
         ]
       }
     }
   ]
   ```

</TabItem>
</Tabs>

:::info
See the [CLI documentation](/cli) for more details about the CLI.
:::

### Change traffic, then "Sniff&Diff"

One of the benefits for using the network mapper is the ability to track changes over time for communication within your
cluster. We call that "Sniff&Diff": continue to sniff traffic, then diff the network mapper output to see what changed.

1. Let's save the current state of traffic from the cluster into a file we will compare against later:
   ```shell
   otterize mapper list -n otterize-tutorial-mapper > intents-original.txt
   ```

2. And now we can add more traffic to the cluster and see how the network mapper tracks it.
   Let's deploy an extension to the example, consisting of two pods &mdash;
   `other-client` and `other-server`, communicating over HTTP:

   ```shell
   kubectl apply -n otterize-tutorial-mapper -f https://docs.otterize.com/code-examples/network-mapper/all-other.yaml
   ```

<details>
<summary>Check that the client and server pods were deployed</summary>

```bash
kubectl get pods -n otterize-tutorial-mapper
```
You should see
```
NAME                      READY   STATUS    RESTARTS   AGE
client-756f7677f8-rdm8l         1/1     Running   0          15m
client2-5c4479947b-cdkl2        1/1     Running   0          9m18s
other-client-8666d97d9c-29xpg   1/1     Running   0          4m8s
other-server-676dbc5f5-bqxs4    1/1     Running   0          4m3s
server-6698c58cbc-tht9n         1/1     Running   0          15m
```
</details>

3. Let's list the network map again:
   ```shell
   otterize mapper list -n otterize-tutorial-mapper
   ```
   You should see all the calls including the new ones:
   ```shell
   client in namespace otterize-tutorial-mapper calls:
     - server
   client2 in namespace otterize-tutorial-mapper calls:
     - server
   # highlight-start
   other-client in namespace otterize-tutorial-mapper calls:
     - other-server
   # highlight-end
      ```
5. We can also compare both outputs to see the difference. Start by saving the updated list to a file:
   ```bash
   otterize mapper list -n otterize-tutorial-mapper > intents-updated.txt
   ```
6. Now compare the original file with the updated file using:
   ```bash
   diff -y intents-original.txt intents-updated.txt;echo
   ```
   Note the new calls identified on the lower right:
   ```bash
   client in namespace otterize-tutorial-mapper calls:             client in namespace otterize-tutorial-mapper calls:
     - server                                                        - server
   client2 in namespace otterize-tutorial-mapper calls:            client2 in namespace otterize-tutorial-mapper calls:
     - server                                                        - server
   # highlight-start
                                                                  > other-client in namespace otterize-tutorial-mapper calls:
                                                                  >   - other-server
   # highlight-end
      ```

## What's next

The network mapper is a great way to bootstrap IBAC. It generates client intents files that reflect
the current topology of your services; those can then be used by each client team to grant them easy
and *secure* access to the services they need, and as their needs evolve, they need only evolve
the intents files. We'll see more of that below.

Where to go next?

- If you haven't already, see the [automate network policies tutorial](/quick-tutorials/k8s-network-policies).
- Or go to the next tutorial to [automate secure access for Kafka](/quick-tutorials/k8s-kafka-mtls).
- You might also dive deeper into the guide for [IBAC with network policies](../guides/k8s-ibac-via-network-policies).
  - It even contains a bootstrapping exercise at the end!
- For a deeper dive into the network mapper, see the guide for [mapping pod-to-pod calls](../guides/k8s-mapping-pod-to-pod-calls).

### Teardown

To remove the deployed examples run:

```bash
kubectl delete namespace otterize-tutorial-mapper
helm uninstall network-mapper -n otterize-system
```
