---
title: Mapping pod-to-pod calls in Kubernetes
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The Network Mapper allows you to map pod-to-pod traffic within your K8s cluster. This tutorial will guide you
through installing Otterize, mapping traffic and tracking changes.

## Install the network mapper

{@include: ../../_common/install-otterize-network-mapper.md}

## Install the Otterize CLI

{@include: ../../_common/install-otterize-cli.md}


## Retrieving the network map (list, YAML intents, and JSON)

{@include: ../../_common/network-mapper/intents-list-export.md}

:::info
See the [Network Mapper documentation](/network-mapper/intro) for more details about the CLI.
:::

### Filtering by namespace
One of the CLI's more useful flags is the namespace filter. You can query the mapper for calls originating only from a specific namespace with
```bash
otterize mapper list -n {NAMESPACE}
```

### Resting the mapper state
The Otterize network mapper keeps track of active connections as long as its running. You can clear its state by calling
```bash
otterize mapper mapper reset
```

For the complete list of the otterize network mapper capabilities as read the [CLI command reference](/cli/#network-mapper).

## Change traffic -> Sniff&Diff

One of the benefits for using the network mapper is the ability to track changes over time for communication within your
cluster.

1. Let's save the current state of traffic from the cluster into a file we will compare against later
   ```shell
   otterize mapper list > intents-original.txt
   ```

2. And now we can add traffic to the cluster and see how the Network Mapper tracks it. You can do that by deploying our
   example
   which consists of two pods: client and server, communicating over HTTP. Deploy example:
   ```shell
   kubectl apply -n otterize-tutorial-mapper -f https://docs.otterize.com/code-examples/network-mapper/all.yaml
   ```

<details>
<summary>Check that the client and server pods were deployed</summary>

```bash
kubectl get pods -n otterize-tutorial-mapper
```
You should see
```
NAME                      READY   STATUS    RESTARTS   AGE
client-756f7677f8-d6qdq   1/1     Running   0          45s
server-6698c58cbc-ssxvx   1/1     Running   0          45s
```
</details>

3. Export the updated observed intents.
   ```shell
   otterize mapper list
   ```
   You will now see the client and server pods communication in addition
   to the previously observed traffic.
   ```shell
   # highlight-start
   client calls:
     - server
   # highlight-end

   checkoutservice in namespace ecom-demo calls:
     - orderservice

   orderservice in namespace ecom-demo calls:
     - kafka
   ```
5. We can also compare both output to see the difference. We'll start by saving the updated state to a file with
   ```bash
   otterize mapper list > intents-updated.txt
   ```
6. And compare the original file with the updated file using
   ```bash
   diff --color=always -y intents-original.txt intents-updated.txt;echo
   ```
   You should see a result looking like
   ```bash
                                                       > client calls:
                                                       >   - server
                                                       >
   checkoutservice in namespace ecom-demo calls:       checkoutservice in namespace ecom-demo calls:
     - orderservice                                      - orderservice

   orderservice in namespace ecom-demo calls:          orderservice in namespace ecom-demo calls:
     - kafka                                             - kafka
      ```


## What calls are picked up

The Otterize Network Mapper creates a map of in-cluster traffic by (1) capturing DNS traffic and (2) inspecting active connections in the same manner `netstat` does, then resolving the IP addresses participating in connections to the Pods, and crawling up the ownership of the Pod until it reaches the root object.
### Active TCP connections
Any two `pods` communicating over TCP have a record on the respective `nodes` they are running on
we can use to identify the connection.
### DNS responses
DNS is a common network protocol used for service discovery. When a `pod` [checkout-service] tries to connect to a `service`
[order-service] or another pod, a DNS query is sent out. The Network Mapper watches DNS responses and extracts the IP addresses

### Service name resolution
Service name resolution is performed one of two ways:

If an otterize/service-name label is present, that name is used.
If not, a recursive look up is performed for the Kubernetes resource owner for a Pod until the root is reached. For example, if you have a Deployment named client, which then creates and owns a ReplicaSet, which then creates and owns a Pod, then the service name for that pod is client - same as the name of the Deployment.

This results in a human-readable name that developers should recognize from other resources (e.g. because it's the name of the `Deployment` resource).

<!-- FIXME
:::tip
Checkout this blog by Evyatar Meged about how we implemented this feature.
:::
-->

## Network mapping for bootstrapping access controls
To export intents from the Otterize Network Mapper, run:
```bash
otterize mapper export
```
The output is concatenated ClientIntents which can be piped to `kubectl apply`.
```yaml
apiVersion: k8s.otterize.com/v1alpha1
kind: ClientIntents
metadata:
  name: frontend
  namespace: otterize-ecom-demo
spec:
  service:
    name: frontend
  calls:
    - name: checkoutservice
      type: HTTP
---
apiVersion: k8s.otterize.com/v1alpha1
kind: ClientIntents
metadata:
  name: checkoutservice
  namespace: otterize-ecom-demo
spec:
  service:
    name: checkoutservice
  calls:
    - name: productcatalogservice
      type: HTTP
```

You can directly apply these ClientIntents to K8s and Otterize will enforce network policies according to them automatically.
:::tip
To learn more about how to use ClientIntents to manage network policies read this [guide](/guides/k8s-ibac-via-network-policies/)
:::

## What's next

- Use your mapped traffic as intents and implement [IBAC via network policies](/guides/k8s-ibac-via-network-policies/).


## Current limitations

* DNS response tracking is relevant for clusters using DNS for service discovery
