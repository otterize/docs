---
sidebar_position: 2
title: Deeper dive into network policies
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

[Network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) are one of the tools we
can use for traffic shaping within K8s clusters. They allow us to shape traffic using selectors, policies, and L3
and L4 identifiers. To enforce network policies, a Kubernetes cluster requires a CNI supporting network policies to be
installed. Some popular options are Calico and Cilium.

## Closer look at a network policy
Let's take a look at an example showing a network policy allowing traffic:
- _from_ pods labeled `app:backend` in namespaces labeled `env:production`
- _to_ pods labeled `app:db` in the namespace  `production-db`

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-production-backend
  # highlight-next-line
  namespace: production-db            # [Target filter] Policy applies to target pods in this namespace
spec:
  podSelector:
    matchLabels:
      # highlight-next-line
      app: db                         # [Target filter] Policy applies to target pods with this label
  policyTypes:
    # highlight-next-line
    - Ingress                         # [Direction] Policy implemented as a filter on incoming connections
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              # highlight-next-line
              env: production         # [Source filter] Policy filters source pods from namespaces with this label
        - podSelector:
            matchLabels:
              # highlight-next-line
              app: backend            # [Source filter] Policy filters source pods from with this label
```

## Setting security scope via default network policies

Two common approaches for working with network policies are

- Allow all traffic between pods, protect some pods by applying ingress network policies to them.
- Block all traffic between pods except allowed traffic by network policies.

You can apply both approaches (allow/block all) within your cluster (e.g. by applying network policies based on
namespaces).

## Default deny network policy
To block all traffic within a namespace (e.g. _production_) you can apply a default deny network policy like the
following example

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: production
spec:
  podSelector: { }
  policyTypes:
    - Ingress
```

## Auto generating network policies for external traffic
The intents operator defaults to automatically generate network policies for `Service` (type `LoadBalancer` and `NodePort`), and Ingress traffic
when an intent will generate a network policy that can block external traffic. To disable this feature, consult the [documentation for the intents operator](/components/intents-operator)


Let's look at an example from our demo. We have a `frontend` service being accessed from multiple sources
- `loadgenerator` calls it from within the cluster to generate traffic, by accessing the `frontend` `ClusterIP Service`.
- `frontend-external` is a `Service` with type `LoadBalancer` directing traffic from outside the cluster to the frontend pods. The `LoadBalancer` type means that a cloud provider load balancer will be created and point traffic from the Internet to these pods.

By applying the following intents file:
```yaml
apiVersion: k8s.otterize.com/v1alpha1
kind: ClientIntents
metadata:
  name: loadgenerator
spec:
  service:
    name: loadgenerator
  calls:
    - name: frontend
```
Otterize will generate a network policy allowing access from the `loadgenerator` service to the `frontend` service.
Once a single network policy matches a pod, other traffic not allowed by existing network policies to the `frontend` will get blocked. In our
case that means that the `frontend-external` `LoadBalancer` won't be able to communicate with `frontend`.

To overcome this, Otterize will automatically generate a network policy to allow traffic from `frontend-external`
to `frontend` by relying on the existence of the `LoadBalancer Service` as an indicator of intent between the two.

Why doesn't Otterize always generate network policies for ingress types?
Because if no network policies exist, automatically generating a network policy to allow `frontend-external` -> `frontend`
would block existing traffic like `loadgenerator` -> `frontend`.



## How intents translate to network policies.
Let's follow an example scenario and track how Otterize configures network policies when we apply intents.
### Deploy example
Our example consists of two pods: an HTTP server and a client that calls it. We also deploy a default-deny ingress network policy,
blocking pods from accepting incoming calls unless another network policy explicitly allows them.

<details>
<summary>Expand to see the example YAML files</summary>
<Tabs>

<TabItem value="namespace.yaml" label="namespace.yaml" default>

   ```yaml
   {@include: ../../../static/code-examples/network-policies/namespace.yaml}
   ```

</TabItem>

<TabItem value="server.yaml" label="server.yaml" default>

   ```yaml
  {@include: ../../../static/code-examples/network-policies/server-deployment.yaml}
   ---
  {@include: ../../../static/code-examples/network-policies/server-service.yaml}
  ```

</TabItem>
<TabItem value="client.yaml" label="client.yaml" default>

   ```yaml
   {@include: ../../../static/code-examples/network-policies/client-deployment.yaml}
   ```

</TabItem>

<TabItem value="default-deny.yaml" label="default-deny.yaml" default>

   ```yaml
   {@include: ../../../static/code-examples/network-policies/default-deny-network-policy.yaml}
   ```

</TabItem>
</Tabs>
</details>

1. Deploy the client, server, and the default deny network policy using `kubectl`.

   ```shell
   kubectl apply -f https://docs.otterize.com/code-examples/network-policies/all.yaml
   ```
2. Check that the `client` and server `pods` were deployed
   ```bash
   kubectl get pods -n otterize-tutorial-npol
   ```
   You should see
   ```
   NAME                      READY   STATUS    RESTARTS   AGE
   client-5689997b5c-grlnt   1/1     Running   0          35s
   server-6698c58cbc-v9n9b   1/1     Running   0          34s
   ```

3. The client intents to call the server are declared with this `intents.yaml` file:

   ```yaml
   {@include: ../../../static/code-examples/network-policies/intents.yaml}
   ```
   Let's apply it:
   ```shell
   kubectl apply -f https://docs.otterize.com/code-examples/network-policies/intents.yaml
   ```
### Track artifacts
After applying the intents file Otterize performed multiple actions to allow access from the client to the server using network policies:
- Create a network policy allowing traffic from the [client, namespace -labeled] pods to [server-labeled] pods
- Label the client pods
- Label the client pod namespaces
- Label the server pods
1. Let's look at the generated network policy:
  ```bash
  kubectl describe networkpolicies -n otterize-tutorial-npol access-to-server-from-otterize-tutorial-npol
  ```
  You should see (without the comments)
  ```yaml
  Name:         access-to-server-from-otterize-tutorial-npol
  # [Target filter] namespace
  # highlight-next-line
  Namespace:    otterize-tutorial-npol
  Created on:   2022-09-08 19:12:24 +0300 IDT
  Labels:       intents.otterize.com/network-policy=true
  Annotations:  <none>
  Spec:
    # [Target filter] pods with this label
    # highlight-next-line
    PodSelector:     intents.otterize.com/server=server-otterize-tutorial-np-7e16db
    Allowing ingress traffic:
      To Port: <any> (traffic allowed to all ports)
      From:
        # [Source filter] namespaces with this label
        # highlight-next-line
        NamespaceSelector: intents.otterize.com/namespace-name=otterize-tutorial-npol
        # [Source filter] pods with this label
        # highlight-next-line
        PodSelector: intents.otterize.com/access-server-otterize-tutorial-np-7e16db=true
    Not affecting egress traffic
    # [Direction]
    # highlight-next-line
    Policy Types: Ingress
  ```
2. And we can also see that the client and server pods are now labeled:
  ```bash
  kubectl get pods -n otterize-tutorial-npol --show-labels
  ```
  You should see
  ```bash
  NAME                      READY   STATUS    RESTARTS   AGE     LABELS
  client-5cb67b748-l25vg    1/1     Running   0          7m57s   intents.otterize.com/access-server-otterize-tutorial-np-7e16db=true,intents.otterize.com/client=true,intents.otterize.com/server=client-otterize-tutorial-np-699302,pod-template-hash=5cb67b748,spire-integration.otterize.com/service-name=client
  server-564b56f596-54str   1/1     Running   0          7m56s   intents.otterize.com/server=server-otterize-tutorial-np-7e16db,pod-template-hash=564b56f596,spire-integration.otterize.com/service-name=server
  ```

  And when we break down the label structure we can see
    - For the server - <span style={{color:'gray'}}>intents.otterize.com/server</span>=<span style={{color:'magenta'}}>server</span>-<span style={{color:'red'}}>otterize-tutorial-np</span>-<span style={{color:'green'}}>7e16db</span>
      - <span style={{color:'gray'}}>intents.otterize.com/server</span> - Label prefix for servers
      - <span style={{color:'magenta'}}>server</span> - Server pod name
      - <span style={{color:'red'}}>otterize-tutorial-np</span> - Server pod namespace (might be truncated)
      - <span style={{color:'green'}}>7e16db</span> - Hash for server pod name and and namespace
    - For the client - <span style={{color:'gray'}}>intents.otterize.com/access</span>-<span style={{color:'magenta'}}>server</span>-<span style={{color:'red'}}>otterize-tutorial-np</span>-<span style={{color:'green'}}>7e16db</span>=true
      - <span style={{color:'gray'}}>intents.otterize.com/access</span> - Label prefix for clients
      - <span style={{color:'magenta'}}>server</span> - Server pod name
      - <span style={{color:'red'}}>otterize-tutorial-np</span> - Server pod namespace (might be truncated)
      - <span style={{color:'green'}}>7e16db</span> - Hash for server pod name and and namespace
3. Finally, let's look at the namespace label with:
  ```bash
  kubectl get namespace otterize-tutorial-npol --show-labels
  ```
  You should see:
  ```bash
  NAME                     STATUS   AGE   LABELS
otterize-tutorial-npol   Active   36s   intents.otterize.com/namespace-name=otterize-tutorial-npol,kubernetes.io/metadata.name=otterize-tutorial-npol
  ```
  With the new label added by Otterize - `intents.otterize.com/namespace-name=otterize-tutorial-npol`
